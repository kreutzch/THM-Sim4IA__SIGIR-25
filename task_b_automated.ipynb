{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed38a5a7",
   "metadata": {},
   "source": [
    "#### Task B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c34f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from llm_hlp import next_queries_gemini, next_queries_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870394dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OpenAI_token.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = content\n",
    "\n",
    "with open('Gemini_token.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    os.environ[\"GEMINI_API_KEY\"] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6376ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "utterances = {}\n",
    "with open('predetermined_utterances_test.json') as f:\n",
    "    utterances = json.load(f)\n",
    "    \n",
    "pairs = {}\n",
    "with open('predetermined_utterance_response_pairs_test.json') as f:\n",
    "    pairs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d4cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = {}\n",
    "for ut_id in utterances:\n",
    "    tmp = []\n",
    "    for ut in utterances[ut_id]:\n",
    "        tmp.append((ut, pairs[ut]))\n",
    "    combined[ut_id] = tmp\n",
    "\n",
    "with open('combined_data_task_B.json', 'w') as f:\n",
    "    json.dump(combined, f, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3271600",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"You are tasked with writing potential next queries for a multiple given queries and already received answers by a conversational search system.\"\n",
    "# change this example prompt\n",
    "# you can also change the instruction IF YOU KNOW WHAT YOU ARE DOING\n",
    "# if not, please just let them be\n",
    "prompt_instructions = 'You are required to return EXACTLY %NUMOFQUERIES% potential next queries, NOTHING ELSE. Queries need to be distinct from each other. Order them in descending order by probability of them being the next query. Return these next %NUMOFQUERIES% queries as a Python-style list. These are the previous queries (preceded by \"?\") as well as the corresponding received responses by the conversational system (preceded by \">\"):\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5963a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set these variables\n",
    "\n",
    "use_gpt = False\n",
    "modelname = 'gemini-2.0-flash' # 'gpt-4.1-nano' # 'gemini-2.0-flash' #'gemini-2.5-flash-preview-05-20' # 'gemini-2.0-flash'\n",
    "\n",
    "name = 'this-is-my-name' \n",
    "strategy = 'This is my strategy ......' +\\\n",
    "    '_____ PROMPT: ' + example_prompt + ' _____ INSTRUCTION: ' + prompt_instructions + ' _____ LLM: ' + modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('task_B_automated_' + modelname + '--' + name + '.json'):\n",
    "    with open('task_B_automated_' + modelname + '--' + name + '.json', 'w') as f:\n",
    "        json.dump({'name': name, 'strategy': strategy}, f, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be53991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('combined_data_task_B.json') as f:\n",
    "    combined_pairs = json.load(f)\n",
    "with open('task_B_automated_' + modelname + '--' + name + '.json') as f:\n",
    "    next_utterances = json.load(f)    \n",
    "    \n",
    "for ut_id in combined_pairs:\n",
    "    if ut_id not in next_utterances or len(next_utterances[ut_id]) < 10:\n",
    "        text_for_llm = ''\n",
    "        for a, b in combined_pairs[ut_id]:\n",
    "            text_for_llm += '?: ' + a + '\\n'\n",
    "            text_for_llm += '>: ' + b + '\\n'\n",
    "            text_for_llm += '--\\n'\n",
    "        \n",
    "        next_new_queries = []\n",
    "        already_entered_queries = []\n",
    "        already_entered_queries_for_llm = ''\n",
    "        if ut_id in next_utterances:\n",
    "            next_new_queries = next_utterances[ut_id]\n",
    "        \n",
    "            for i in range(len(next_new_queries)):\n",
    "                already_entered_queries.append(next_new_queries[i])\n",
    "                already_entered_queries_for_llm += next_new_queries[i] + ', '\n",
    "                \n",
    "        if len(already_entered_queries) > 0:\n",
    "            curr_prompt_instructions = ' These queries have already been written as potential next ones, YOU CANNOT REPEAT THEM: ' + already_entered_queries_for_llm[:-2] + '. ' + prompt_instructions\n",
    "        else:\n",
    "            curr_prompt_instructions = prompt_instructions\n",
    "        \n",
    "        print(example_prompt + ' ' + curr_prompt_instructions.replace('%NUMOFQUERIES%', str(10 - len(already_entered_queries))) + ' ' + text_for_llm)\n",
    "\n",
    "        if use_gpt:\n",
    "            potential_next_utterances, error_occurred = next_queries_gpt(text_for_llm, example_prompt, 10 - len(already_entered_queries), curr_prompt_instructions, modelname)\n",
    "        else:\n",
    "            potential_next_utterances, error_occurred = next_queries_gemini(text_for_llm, example_prompt, 10 - len(already_entered_queries), curr_prompt_instructions, modelname)\n",
    "            \n",
    "        for new_query in potential_next_utterances:\n",
    "            if len(next_new_queries) < 11:\n",
    "                next_new_queries.append(new_query)\n",
    "        next_utterances[str(ut_id)] = next_new_queries\n",
    "                \n",
    "        with open('task_B_automated_' + modelname + '--' + name + '.json', 'w') as f:\n",
    "            json.dump(next_utterances, f, indent = 2)\n",
    "    \n",
    "        print('These ' + str(len(next_new_queries)) + ' options have been entered by the LLM as potential next queries:')\n",
    "        for i in range(len(next_new_queries)):\n",
    "            print(str(i + 1) + ' -- ' + next_new_queries[i])\n",
    "        \n",
    "        if len(potential_next_utterances) + len(already_entered_queries) != 10:\n",
    "            print('Please run this query again, there have only been ' + str(len(next_new_queries)) + ' queries.')\n",
    "\n",
    "        print('Thanks!')\n",
    "        break\n",
    "\n",
    "if len(next_utterances) == 35:\n",
    "    print('Nice job, well done! Please send your `task_B_automated_' + modelname + '--' + name + '.json` file to Christin. Thank you so much! :)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
